# -*- coding: utf-8 -*-
"""ittai's_expiriment_phases.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JY-C46L9JFR7RiytbFbqFCxLySPXk1On

# Exersice 13 : Structured Latent Representations Through Predictive Learning
## Neural Systems Fall 2025
## ETH Zürich
### Exercise Teaching Assistants:
- Julien Schmidt (julies1@ethz.ch)
- Raimon Bullich (rbullich@ethz.ch)
"""

!pip install ratinabox

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim import RMSprop
from torch.utils.data import DataLoader, TensorDataset, random_split
import torch.nn.init as init
import matplotlib.pyplot as plt
import matplotlib
from matplotlib.collections import EllipseCollection
from sklearn.decomposition import PCA
from tqdm import tqdm
from ratinabox.Environment import Environment
from ratinabox.Agent import Agent
from ratinabox.Neurons import FieldOfViewBVCs, FieldOfViewOVCs
from ratinabox import utils
import ratinabox
import warnings
warnings.filterwarnings('ignore')

"""# Section 1 : Structured latent learning in simple discrete environment"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device = "cpu" #we force cpu because this dataset is small and colab GPU compute is limited. Feel free to change :)
print(device)

"""### Prepating our dataset

### Model with access to action information

### Model without access to action information

### Background code for agent and environment
In this notebook, we replicate an environment similar to the 2D environment used in Recanatesi et al. You dont need to go through this code. Its simply background code to run the simulation. Just run it and move to the next step.
##### (MAKE SURE TO RUN)
"""

# THIS IS BACKGROUND CODE FOR INITIALIZING THE AGENT AND ITS VISION CELLS. YOU DO NOT NEED TO UNDERSTAND THIS.
# The class below is responsible for generating field of view cells which are fixed to the wall and activate allong with the
# agent in whichever direction it is facing. If you are curious, the way this class works is by commandeering the Field of View Object Vector Cells
# class in rat in a box and forcing the distance of the cells' receptive fields relative to the agent so that they intersect with the walls of the environment.
# This is a crude but effective way of replicting Recanatesi et al.'s 2D environment and agent.

# Imagine in whichever dierction the agent is facing, it shoots out 6 beams of light at different angles relative to its nose. These beams of light refect off
# the walls and and return to the agent the quantity of red, green and blue at that point of the wall along with the agents distance to the wall at that same
# intersection point. So, for each beam of light, we get 4 values: [R,G,B,distance]. We have 6 beams of beams of light so we get a total of 6*4=24 observations
# for each timestep. These serve as our observations in the environment.

class WallFixedFOVc(FieldOfViewOVCs):
    def __init__(self, Agent, params={}):
        super().__init__(Agent,params)
        self.history["tuning_distances"] = []
        self.history["sigma_distances"] = []
        self.history["sigma_angles"] = []
        self.history["dists"] = []

    def display_vector_cells(self,
                         fig=None,
                         ax=None,
                         t=None,
                         **kwargs):
        """Visualises the current firing rate of these cells relative to the Agent.
        Essentially this plots the "manifold" ontop of the Agent.
        Each cell is plotted as an ellipse where the alpha-value of its facecolor reflects the current firing rate
        (normalised against the approximate maximum firing rate for all cells, but, take this just as a visualisation).
        Each ellipse is an approximation of the receptive field of the cell which is a von Mises distribution in angule and a Gaussian in distance.
        The width of the ellipse in r and theta give 1 sigma of these distributions (for von Mises: kappa ~= 1/sigma^2).

        This assumes the x-axis in the Agent's frame of reference is the heading direction.
        (Or the heading diection is the X-axis for egocentric frame of reference). IN this case the Y-axis is the towards the "left" of the agent.

        Args:
        • fig, ax: the matplotlib fig, ax objects to plot on (if any), otherwise will plot the Environment
        • t (float): time to plot at
        • object_type (int): if self.cell_type=="OVC", which object type to plot

        Returns:
            fig, ax: with the
        """
        if t is None:
            t = self.Agent.history["t"][-1]
        t_id = np.argmin(np.abs(np.array(self.Agent.history["t"]) - t))

        if fig is None and ax is None:
            fig, ax = self.Agent.plot_trajectory(t_start=t - 10, t_end=t, **kwargs)

        pos = self.Agent.history["pos"][t_id]

        y_axis_wrt_agent = np.array([0, 1])
        x_axis_wrt_agent = np.array([1,0])
        head_direction = self.Agent.history["head_direction"][t_id]
        head_direction_angle = 0.0


        if self.reference_frame == "egocentric":
            head_direction = self.Agent.history["head_direction"][t_id]
            # head direction angle (CCW from true North)
            head_direction_angle = (180 / np.pi) * ratinabox.utils.get_angle(head_direction)

            # this assumes the "x" dimension is the agents head direction and "y" is to its left
            x_axis_wrt_agent = head_direction / np.linalg.norm(head_direction)
            y_axis_wrt_agent = utils.rotate(x_axis_wrt_agent, np.pi / 2)


        fr = np.array(self.history["firingrate"][t_id])

        tuning_distances = np.array(self.history["tuning_distances"][t_id])
        sigma_angles = np.array(self.history["sigma_angles"][t_id])
        sigma_distances = np.array(self.history["sigma_distances"][t_id])
        tuning_angles = self.tuning_angles #this is unchanged


        x = tuning_distances * np.cos(tuning_angles)
        y = tuning_distances * np.sin(tuning_angles)

        pos_of_cells = pos + np.outer(x, x_axis_wrt_agent) + np.outer(y, y_axis_wrt_agent)

        ww = sigma_angles * tuning_distances
        hh = sigma_distances
        aa  = 1.0 * head_direction_angle + tuning_angles * 180 / np.pi

        ec = EllipseCollection(ww,hh, aa, units = 'x',
                                offsets = pos_of_cells,
                                offset_transform = ax.transData,
                                linewidth=0.5,
                                edgecolor="dimgrey",
                                zorder = 2.1,
                                )
        if self.cell_colors is None:
            facecolor = self.color if self.color is not None else "C1"
            facecolor = np.array(matplotlib.colors.to_rgba(facecolor))
            facecolor_array = np.tile(np.array(facecolor), (self.n, 1))
        else:
            facecolor_array = self.cell_colors.copy() #made in child class init. Each cell can have a different plot color.
            # e.g. if cells are slective to different object types or however you like
        facecolor_array[:, -1] = 0.7*np.maximum(
            0, np.minimum(1, fr / (0.5 * self.max_fr))
        ) # scale alpha so firing rate shows as how "solid" (up to 0.7 so you can _just_ seen whats beneath) to color of this vector cell is.
        ec.set_facecolors(facecolor_array)
        ax.add_collection(ec)

        return fig, ax


    def ray_distances_to_walls(self, agent_pos, head_direction, thetas, ray_length, walls):
        # This function computes the distance the agent is to the wall relative to its 6 beams of light. We provide the agent position, its
        # head direction and the 6 beams angles. We get as an output the wall distance of each beam relative to the agents position and HD.
        n = len(thetas)

        heading = np.arctan2(head_direction[1], head_direction[0])

        #print(heading)
        # 1) Build the end points of each ray: shape (n,2)
        ends = np.stack([agent_pos + ray_length * np.array([np.cos(heading + θ),np.sin(heading + θ)]) for θ in thetas], axis=0)

        # 3) starts is just the agent_pos repeated: shape (n,2)
        starts = np.tile(agent_pos[np.newaxis, :], (n, 1))

        # 4) build segs as (n_rays, 2, 2), with [p0, p1] = [start, end]
        segs = np.zeros((n, 2, 2))
        segs[:, 0, :] = starts  # p_a0 = start
        segs[:, 1, :] = ends    # p_a1 = end

        # 5) get the raw line parameters
        intercepts = utils.vector_intercepts(segs, walls, return_collisions=False)
        la = intercepts[..., 0]
        lb = intercepts[..., 1]

        # 6) mask for real segment–segment hits
        valid = (la > 0) & (la < 1) & (lb > 0) & (lb < 1)

        # 7) pick the nearest hit along each ray (smallest la)
        la_valid = np.where(valid, la, np.inf)  # invalid → ∞
        min_la = la_valid.min(axis=1)           # one per ray

        # 8) convert parameter to distance
        distances = np.minimum(min_la * ray_length, ray_length)
        #print(distances)
        return distances #as 6x1 vector of distances for each of the 6 sensors

    def update_cell_locations(self):
        # we update the cell locations by forcing them to remain on the walls.
        # A cell should never wander across the open field as they were originally
        # indended to by rat in a box by default.
        thetas = self.Agent.Neurons[0].tuning_angles
        n=len(thetas)

        self.dists = self.ray_distances_to_walls(self.Agent.pos, self.Agent.head_direction, thetas, 100.0, np.array(self.Agent.Environment.walls))

        s = self.dists*16.5
        self.tuning_distances = np.ones(n)*0.06*s
        self.sigma_distances = np.ones(n)*0.05
        self.sigma_angles = np.ones(n)*0.83333333/s

    def save_to_history(self):
        super().save_to_history()  # Save standard history
        self.history["tuning_distances"].append(self.tuning_distances.copy())
        self.history["sigma_distances"].append(self.sigma_distances.copy())
        self.history["sigma_angles"].append(self.sigma_angles.copy())
        self.history["dists"].append(self.dists.copy()/np.linalg.norm([x_ratio, y_ratio])) #we divide by sqrt(2) because the model wants numbers between 0 and 1 and the max distance in a 1x1 square is sqrt(2)

    def reset_history(self):
        super().reset_history()
        self.history["dists"]=[]

def get_next_move(
    pos, hd_angle, Env, *,
    # --- step length ---
    step_len_fixed=None,                 # e.g. 0.04 → fixed; None → sample
    mu_len=0.02, sigma_len=0.05,         # used only if fixed is None (trunc. Normal)
    # --- angle sampling ---
    angle_kappa=6.0,                     # von Mises concentration (higher = straighter)
    angle_dist='vonmises',               # 'vonmises' | 'uniform' | 'keep'
    # --- logging / numerics ---
    encode_relative_angles=True,         # True → log Δangle (theta - hd_angle)
    normalize_dist=True,                 # True → distance_used / max(box_w, box_h)
    eps=1e-9,                            # geometric epsilon
    detach_frac=1e-3,                    # post-hit nudge into interior (fraction of box)
    corner_tol_frac=1e-9,                # tx≈ty tolerance for "corner" hit
    jitter_after_hit_deg=0.0,            # tiny random deflection after a hit (0 = none)
):
    """
    Continuous stepper:
      • sample an executed angle (theta),
      • attempt a step; clip to first wall (no in-frame bounce),
      • log [angle, distance_used] where angle is RELATIVE if encode_relative_angles=True,
      • next heading = 180° turn if clipped, with anti-sticking nudge/projection.
    Returns:
      next_pos : (2,) float
      next_angle : float radians in [-π,π)
      action_vec : (2,) float = [ angle_to_log , distance_used_(norm or raw) ]
    """
    import numpy as np

    # ---- box extents ---------------------------------------------------------
    B = np.asarray(Env.boundary) if hasattr(Env, "boundary") else np.asarray(Env.params["boundary"])
    xmin, ymin = B[:, 0].min(), B[:, 1].min()
    xmax, ymax = B[:, 0].max(), B[:, 1].max()
    W, H = xmax - xmin, ymax - ymin
    box_side = max(W, H)
    corner_tol  = corner_tol_frac * box_side
    detach_eps  = detach_frac * box_side

    def _wrap(a):                      # [-π, π)
        return (a + np.pi) % (2*np.pi) - np.pi

    def _ensure_interior_angle(theta, p, margin=5*eps):
        """If theta points outward at a boundary contact, flip offending component(s)."""
        vx, vy = np.cos(theta), np.sin(theta)
        if p[0] <= xmin + margin and vx < 0: vx = abs(vx)
        if p[0] >= xmax - margin and vx > 0: vx = -abs(vx)
        if p[1] <= ymin + margin and vy < 0: vy = abs(vy)
        if p[1] >= ymax - margin and vy > 0: vy = -abs(vy)
        return np.arctan2(vy, vx)

    # ---- 1) executed angle this frame (theta) --------------------------------
    if angle_dist == 'vonmises':
        theta = _wrap(hd_angle + np.random.vonmises(0.0, angle_kappa))
    elif angle_dist == 'uniform':
        theta = np.random.uniform(-np.pi, np.pi)
    else:  # 'keep'
        theta = _wrap(hd_angle)
    v = np.array([np.cos(theta), np.sin(theta)], float)

    # ---- 2) requested step length -------------------------------------------
    if step_len_fixed is not None:
        step_req = float(step_len_fixed)
    else:
        step_req = np.random.normal(mu_len, sigma_len)
        while step_req <= 0:
            step_req = np.random.normal(mu_len, sigma_len)

    # ---- 3) clip to first wall (no in-frame bounce) --------------------------
    tx = np.inf
    if   v[0] > 0: tx = (xmax - pos[0]) / v[0]
    elif v[0] < 0: tx = (xmin - pos[0]) / v[0]
    ty = np.inf
    if   v[1] > 0: ty = (ymax - pos[1]) / v[1]
    elif v[1] < 0: ty = (ymin - pos[1]) / v[1]

    # collision classification
    collided = step_req >= min(tx, ty) - eps
    if collided:
        if abs(tx - ty) <= corner_tol:
            t_hit = min(tx, ty); hit_x = hit_y = True
        elif tx < ty:
            t_hit = tx; hit_x, hit_y = True, False
        else:
            t_hit = ty; hit_x, hit_y = False, True
    else:
        t_hit = np.inf; hit_x = hit_y = False

    dist_exec = min(step_req, max(0.0, t_hit - eps))
    next_pos  = pos + dist_exec * v
    # hard clamp
    next_pos[0] = np.clip(next_pos[0], xmin + eps, xmax - eps)
    next_pos[1] = np.clip(next_pos[1], ymin + eps, ymax - eps)

    # ---- 4) next frame's heading --------------------------------------------
    if collided:
        next_angle = _ensure_interior_angle(_wrap(theta + np.pi), next_pos)
        if jitter_after_hit_deg > 0:
            jitter = np.deg2rad(jitter_after_hit_deg) * (2*np.random.rand() - 1.0)
            next_angle = _ensure_interior_angle(_wrap(next_angle + jitter), next_pos)
        # detach slightly from boundary along the new heading (anti-sticking)
        u = np.array([np.cos(next_angle), np.sin(next_angle)], float)
        next_pos = next_pos + detach_eps * u
        next_pos[0] = np.clip(next_pos[0], xmin + eps, xmax - eps)
        next_pos[1] = np.clip(next_pos[1], ymin + eps, ymax - eps)
    else:
        next_angle = theta

    # ---- 5) action to log/train on ------------------------------------------
    logged_angle = _wrap(theta - hd_angle) if encode_relative_angles else theta
    logged_dist  = (dist_exec / box_side) if normalize_dist else dist_exec
    action_vec   = np.array([logged_angle, logged_dist], float)

    return next_pos, next_angle, action_vec

"""### Initialize agent and environment"""

from ratinabox.Environment import Environment
import numpy as np

def rescale_env_with_locations(old_env,
                               t0_locations, t1_locations, t2_locations,
                               x_ratio=1.0, y_ratio=1.0):
    """
    Stretch `old_env` by (x_ratio, y_ratio) and re‑insert the three
    colour‑specific location arrays at rescaled coordinates.

    Parameters
    ----------
    old_env : ratinabox.Environment
        The source rectangular arena.
    t0_locations, t1_locations, t2_locations : (N,2) NumPy arrays
        The (x,y) coordinates of red, green, and purple pucks *from the
        old arena*.
    x_ratio, y_ratio : float
        How much to multiply the width and height.

    Returns
    -------
    new_env  : ratinabox.Environment
        A brand‑new Environment whose boundary and objects are resized.
    new_t0, new_t1, new_t2 : NumPy arrays
        The three location arrays after rescaling (useful for plotting).
    """

    # -- 1 · recover old width & height ----------------------------------
    H_old = old_env.params["scale"]                 # original “height”
    W_old = H_old * old_env.params["aspect"]        # original “width”

    # -- 2 · make the stretched boundary --------------------------------
    H_new, W_new = H_old * y_ratio, W_old * x_ratio
    new_env = Environment({"boundary": [[0, 0], [W_new, 0],
                                        [W_new, H_new], [0, H_new]]})

    # -- 3 · rescale the three colour‑arrays -----------------------------
    scale_vec = np.array([x_ratio, y_ratio])
    new_t0 = t0_locations * scale_vec
    new_t1 = t1_locations * scale_vec
    new_t2 = t2_locations * scale_vec

    # -- 4 · add objects to the new environment --------------------------
    for p in new_t0:
        new_env.add_object(p, type=0)
    for p in new_t1:
        new_env.add_object(p, type=1)
    for p in new_t2:
        new_env.add_object(p, type=2)

    return new_env, new_t0, new_t1, new_t2

VIS_CUTOFF = 0.2
x_ratio, y_ratio = 1.0, 1.0

# Create environment and agent
Env = Environment({"scale": 1, "aspect": 1})
number_of_wall_objects = 12
# objs type 0: RED OBJECTS
t0_locations = np.concatenate((np.round(np.random.uniform(low=0.0,high=1.0,size=(number_of_wall_objects,1))),(np.random.uniform(low=0.0,high=1.0,size=(number_of_wall_objects,1)))),axis=1)
t0_locations = np.where(np.random.rand(len(t0_locations), 1) < 0.5, t0_locations, t0_locations[:, ::-1])  # randomly swap columns
[Env.add_object(l, type=0) for l in t0_locations]
# objs type 1: GREEN OBJECTS
t1_locations = np.concatenate((np.round(np.random.uniform(low=0.0,high=1.0,size=(number_of_wall_objects,1))),(np.random.uniform(low=0.0,high=1.0,size=(number_of_wall_objects,1)))),axis=1)
t1_locations = np.where(np.random.rand(len(t1_locations), 1) < 0.5, t1_locations, t1_locations[:, ::-1])  # randomly swap columns
[Env.add_object(l, type=1) for l in t1_locations]
# objs type 2: PURPLE OBJECTS
t2_locations = np.concatenate((np.round(np.random.uniform(low=0.0,high=1.0,size=(number_of_wall_objects,1))),(np.random.uniform(low=0.0,high=1.0,size=(number_of_wall_objects,1)))),axis=1)
t2_locations = np.where(np.random.rand(len(t2_locations), 1) < 0.5, t2_locations, t2_locations[:, ::-1])  # randomly swap columns
[Env.add_object(l, type=2) for l in t2_locations]
Env= rescale_env_with_locations(Env,t0_locations,t1_locations,t2_locations,x_ratio,y_ratio)[0]
Env.plot_environment()


Ag = Agent(Env)

FoV_OVCs_t0 = WallFixedFOVc(Ag, params={"object_tuning_type":0,"distance_range": [0.01, .11],
        "spatial_resolution": 0.05,  # resolution of each OVC tiling FoV
        "cell_arrangement": "uniform_manifold",
        "angle_range": [0,125]})
FoV_OVCs_t1 = WallFixedFOVc(Ag, params={"object_tuning_type":1,"distance_range": [0.01, .11],
        "spatial_resolution": 0.05,  # resolution of each OVC tiling FoV
        "cell_arrangement": "uniform_manifold",
        "angle_range": [0,125]})
FoV_OVCs_t2 = WallFixedFOVc(Ag, params={"object_tuning_type":2,"distance_range": [0.01, .11],
        "spatial_resolution": 0.05,  # resolution of each OVC tiling FoV
        "cell_arrangement": "uniform_manifold",
        "angle_range": [0,125]})

fov_downscalar = 3

tile_len = 1/64
m_y = int(round(y_ratio / tile_len))
m_x = int(round(x_ratio / tile_len))

grid_centers = np.array([
    [((ix+0.5)*tile_len, (iy+0.5)*tile_len) for iy in range(m_y)]
    for ix in range(m_x)
])

# Initialize for continuous movement
hd_angle = float(np.random.uniform(-np.pi, np.pi))
Ag.pos = np.array([np.random.uniform(0.05, x_ratio-0.05), np.random.uniform(0.05, y_ratio-0.05)], float)
Ag.head_direction = np.array([np.cos(hd_angle), np.sin(hd_angle)], float)
Ag.dt = 0.05

actions_saved = []
for _ in range(1000):
    next_pos, hd_angle, action = get_next_move(Ag.pos, hd_angle, Env)

    Ag.use_DRIFT = False
    Ag.update(forced_next_position=next_pos)
    # set the heading DIRECTLY from the returned next angle
    Ag.head_direction = np.array([np.cos(hd_angle), np.sin(hd_angle)], float)
    Ag.history["head_direction"][-1] = Ag.head_direction.tolist()
    actions_saved.append(action)

    FoV_OVCs_t0.update_cell_locations(); FoV_OVCs_t0.update()
    FoV_OVCs_t1.update_cell_locations(); FoV_OVCs_t1.update()
    FoV_OVCs_t2.update_cell_locations(); FoV_OVCs_t2.update()

print("Please ignore the delay in the agents movements and corresponding observations in this annimation.")
print("The official Ratinabox code is slightly bugged and causes deays. You should at least get an idea of \nhow the agent moves and gathers observations. Plotted here are only red observations but there \nexist also purple and green observations at the same cell locations.")
Ag.animate_trajectory(t_end=3, speed_up=1, additional_plot_func=FoV_OVCs_t0.display_vector_cells)



"""### Generate the training dataset"""

#Here we generate a dataset. Rather than training on one long continuous episode, we break it down into many episodes of 100 timesteps.

n_trials = 1000

timesteps = 100 # lenght of each sequence

act_array = np.zeros((n_trials, timesteps, 2))  # 2-dim actions: [angle, distance]
obs_array = np.zeros((n_trials, timesteps, 24)) # num cells of each FOV group

# loop each episode
for nt in tqdm(np.arange(n_trials)):

    Ag.reset_history() # reset history for every new simulation
    FoV_OVCs_t0.reset_history()
    FoV_OVCs_t1.reset_history()
    FoV_OVCs_t2.reset_history()

    # random start - continuous position
    Ag.pos = np.array([np.random.uniform(0.05, x_ratio-0.05), np.random.uniform(0.05, y_ratio-0.05)], float)
    hd_angle = float(np.random.uniform(-np.pi, np.pi))
    Ag.head_direction = np.array([np.cos(hd_angle), np.sin(hd_angle)], float)
    Ag.dt = 0.05

    actions_saved = []

    # simulate entire episode
    for t in range(int(timesteps)):
        next_pos, hd_angle, action = get_next_move(Ag.pos, hd_angle, Env)
        Ag.use_DRIFT = False
        Ag.update(forced_next_position=next_pos)
        # set the heading DIRECTLY from the returned next angle
        Ag.head_direction = np.array([np.cos(hd_angle), np.sin(hd_angle)], float)
        Ag.history["head_direction"][-1] = Ag.head_direction.tolist()
        actions_saved.append(action)

        FoV_OVCs_t0.update_cell_locations()
        FoV_OVCs_t0.update()
        FoV_OVCs_t1.update_cell_locations()
        FoV_OVCs_t1.update()
        FoV_OVCs_t2.update_cell_locations()
        FoV_OVCs_t2.update()

    # store trial data
    act_array[nt] = np.array(actions_saved)
    obs_array[nt] = np.concatenate([FoV_OVCs_t0.get_history_arrays()['firingrate'],
                                    FoV_OVCs_t1.get_history_arrays()['firingrate'],
                                    FoV_OVCs_t2.get_history_arrays()['firingrate'],
                                    FoV_OVCs_t0.get_history_arrays()['dists']], axis=1)


obs_array_orig = obs_array.copy()

import numpy as np

def masked_copy_noisy(
        arr,
        vis_cutoff,
        noise_std,
        *,
        noise_phase="pre",            # "pre", "post", "both", "none"
        dist_power=4,                  # positive → gain ∝ 1/d^power
        eps=1e-3,                      # clamp to avoid 0
        clip_max=None,
        mask_colours_with_vis=True,
        zero_all_distances=False,
        zero_colours=None,
        inplace=False,
        no_vis_val=0.0,
        rng=None,
        # --- NEW ---
        norm_mode="analytic",          # "analytic", "minmax", or None
        norm_axis=-1                   # for "minmax": axis to reduce over (usually -1 or last distance axis)
):
    """
    arr shape: (..., 24) = 6 sensors × (3 colours + 1 distance)
       [R0 G0 B0  R1 G1 B1  ... R5 G5 B5 | D0 D1 D2 D3 D4 D5]
    """

    if arr.shape[-1] != 24:
        raise ValueError("Expected last dimension = 24")

    gen = rng if rng is not None else np.random
    target = arr if inplace else arr.copy()

    colours = target[..., :18].reshape(*target.shape[:-1], 6, 3)
    dists   = target[..., -6:]

    # 1) Visibility mask
    far = dists > vis_cutoff
    dists[far] = no_vis_val
    if mask_colours_with_vis:
        colours[far] = no_vis_val

    # 2) Optional zeroing
    if zero_all_distances:
        dists[...] = no_vis_val

    if zero_colours:
        if isinstance(zero_colours, str):
            zero_colours = [zero_colours]
        idx_map = {'r': 0, 'g': 1, 'b': 2}
        zero_colours = [c.lower() for c in zero_colours]
        if 'all' in zero_colours:
            colours[...] = no_vis_val
        else:
            for c in zero_colours:
                colours[..., idx_map[c]] = no_vis_val

    # ---- Noise BEFORE transform ----
    if noise_std > 0 and noise_phase in ("pre", "both"):
        dists += gen.normal(0.0, noise_std, size=dists.shape)

    # 3) Fish-like distance transform
    d_safe = np.maximum(dists, eps)
    gain_raw = ((vis_cutoff + eps) / (d_safe + eps)) ** dist_power  # =1 at vis_cutoff

    # 4) Optional clipping
    if clip_max is not None:
        gain_raw = np.minimum(gain_raw, clip_max)

    # 5) Normalize to 0–1 if requested
    if norm_mode == "analytic":
        # gain at "near" (≈ eps) is max; at vis_cutoff is 1
        g_far  = 1.0  # by construction
        g_near = ((vis_cutoff + eps) / (eps + eps)) ** dist_power
        denom = max(g_near - g_far, 1e-12)
        gain = (gain_raw - g_far) / denom          # far→0, near→1
        gain = np.clip(gain, 0.0, 1.0)
    elif norm_mode == "minmax":
        # reduce over norm_axis (default: last axis = distances only if you slice first)
        g_min = gain_raw.min(axis=norm_axis, keepdims=True)
        g_max = gain_raw.max(axis=norm_axis, keepdims=True)
        denom = np.maximum(g_max - g_min, 1e-12)
        gain = (gain_raw - g_min) / denom
    else:
        gain = gain_raw  # no normalization

    dists[...] = gain

    # ---- Noise AFTER transform ----
    if noise_std > 0 and noise_phase in ("post", "both"):
        target += gen.normal(0.0, noise_std, size=target.shape)

    return target

#obs_array = masked_copy_noisy(obs_array, VIS_CUTOFF, inplace=True,noise_std=0)

"""### Generate testing dataset"""

tmp = 50000        # length of simulation

Ag.reset_history()
FoV_OVCs_t0.reset_history()
FoV_OVCs_t1.reset_history()
FoV_OVCs_t2.reset_history()

# random start - continuous position
Ag.pos = np.array([np.random.uniform(0.05, x_ratio-0.05), np.random.uniform(0.05, y_ratio-0.05)], float)
hd_angle = float(np.random.uniform(-np.pi, np.pi))
Ag.head_direction = np.array([np.cos(hd_angle), np.sin(hd_angle)], float)
Ag.dt = 0.05

actions_saved = []

# simulate one trial
for t in tqdm(np.arange(int(tmp))): # time_simulation/Ag.dt gives you all timesteps that the simulation will run.
        # update agent location using continuous movement
        next_pos, hd_angle, action = get_next_move(Ag.pos, hd_angle, Env)
        Ag.use_DRIFT = False
        Ag.update(forced_next_position=next_pos)
        # set the heading DIRECTLY from the returned next angle
        Ag.head_direction = np.array([np.cos(hd_angle), np.sin(hd_angle)], float)
        Ag.history["head_direction"][-1] = Ag.head_direction.tolist()
        actions_saved.append(action)

        FoV_OVCs_t0.update_cell_locations()
        FoV_OVCs_t0.update()
        FoV_OVCs_t1.update_cell_locations()
        FoV_OVCs_t1.update()
        FoV_OVCs_t2.update_cell_locations()
        FoV_OVCs_t2.update()

        # store trial data

obs_array_test = np.concatenate([FoV_OVCs_t0.get_history_arrays()['firingrate'],
                                FoV_OVCs_t1.get_history_arrays()['firingrate'],
                                FoV_OVCs_t2.get_history_arrays()['firingrate'],
                                FoV_OVCs_t0.get_history_arrays()['dists']], axis=1)
obs_array_test_orig = obs_array_test.copy()

#obs_array_test = masked_copy_noisy(obs_array_test, VIS_CUTOFF, inplace=True,noise_std=0)
act_array_test = np.array(actions_saved)  # shape (T, 2) - now 2D actions
# Get positions from ratinabox simulations / check shape of hidden activity
positions = Ag.get_history_arrays()['pos']
Ag.plot_trajectory()

# Check for NaNs
nan_mask = np.isnan(obs_array_test)
has_nan = nan_mask.any().item() # Are there any NaNs?
nan_count = nan_mask.sum().item() # Count of NaNs
nan_indices = np.argwhere(nan_mask)
print("Nan sanity check:",has_nan, nan_count, nan_indices)
obs_array_test= np.nan_to_num(obs_array_test, nan=0.0)

# Convert to torch tensors
obs_seq_test = torch.tensor(obs_array_test, dtype=torch.float32).unsqueeze(1)      # (T, obs_dim)
act_seq_test = torch.tensor(act_array_test, dtype=torch.float32).unsqueeze(1)      # (T, 2) - 2D actions

fr0 = FoV_OVCs_t0.get_history_arrays()['firingrate']
fr1 = FoV_OVCs_t1.get_history_arrays()['firingrate']
fr2 = FoV_OVCs_t2.get_history_arrays()['firingrate']
dst = FoV_OVCs_t0.get_history_arrays()['dists']

print("NaNs fr0:", np.isnan(fr0).sum())
print("NaNs fr1:", np.isnan(fr1).sum())
print("NaNs fr2:", np.isnan(fr2).sum())
print("NaNs dst:", np.isnan(dst).sum())

"""## Defining the Recurrent Neural Network

Here you will have to complete the forward function of the network.

"""

class NormReLU(nn.Module):
    # Implemented as described in Levinstein et al.
    def __init__(self, hidden_size, epsilon=1e-2, noise_std=0.03):
        super().__init__()
        self.bias = nn.Parameter(torch.zeros(hidden_size))  # learnable bias
        self.epsilon = epsilon
        self.noise_std = noise_std

    def forward(self, x):
        x_norm = (x - x.mean(dim=-1, keepdim=True)) / (x.std(dim=-1, keepdim=True) + self.epsilon)
        noise = torch.randn_like(x) * self.noise_std
        return F.relu(x_norm + self.bias + noise)

class HardSigmoid(nn.Module):
    # Implemented as described in Recanatesi et al. supplementary material
    def __init__(self):
        super(HardSigmoid, self).__init__()
        self.g = torch.nn.ReLU()

    def forward(self, x):
        return torch.clamp(0.2 * x + 0.5, min=0.0, max=1.0)

class NextStepRNN(nn.Module):
    def __init__(self, obs_dim=144, act_dim=2, hidden_dim=500):  # Changed default act_dim from 13 to 2
        super().__init__()
        # Save dimensions
        self.obs_dim = obs_dim
        self.act_dim = act_dim
        self.hidden_dim = hidden_dim

        # Layers
        self.W_in = nn.Linear(obs_dim, hidden_dim, bias=False)
        self.W_act = nn.Linear(act_dim, hidden_dim, bias=False)
        self.W_rec = nn.Linear(hidden_dim, hidden_dim, bias=False)
        self.W_out = nn.Linear(hidden_dim, obs_dim)  # Output = next observation
        self.beta = nn.Parameter(torch.zeros(1))
        self.norm_relu = NormReLU(hidden_dim)
        self.sigmoid = nn.Sigmoid()
        self.dropout = nn.Dropout(p=0.15)  # 15% input dropout (Table 1)
        self.hardsigmoid = HardSigmoid()

        # We can use different activation functions.
        #self.g = torch.tanh
        self.g = self.norm_relu # From Levenstein et al.
        #self.g = self.hardsigmoid # From Recanatesi et al.

        self.init_weights()

    #Init weights like in Levenstein et al.
    def init_weights(self):
        tau = 2.0
        k_in = 1.0 / np.sqrt(self.obs_dim + self.act_dim)
        k_out = 1.0 / np.sqrt(self.hidden_dim)

        init.uniform_(self.W_in.weight, -k_in, k_in)
        init.uniform_(self.W_act.weight, -k_in, k_in)
        init.uniform_(self.W_out.weight, -k_out, k_out)

        # Identity-initialized + uniform recurrent weights
        W_rec_data = torch.empty(self.hidden_dim, self.hidden_dim)
        init.uniform_(W_rec_data, -k_out, k_out)
        identity_boost = torch.eye(self.hidden_dim) * (1 - 1 / tau)
        W_rec_data += identity_boost
        self.W_rec.weight.data = W_rec_data
    '''
    #Init weights like in Recanatesi et al.
    def init_weights(self):
        # Initialize W_rec to identity matrix
        nn.init.eye_(self.W_rec.weight)

        # Initialize W_in, W_act, and W_out to normal distribution (mean=0, std=0.02)
        nn.init.normal_(self.W_in.weight, mean=0.0, std=0.02)
        nn.init.normal_(self.W_act.weight, mean=0.0, std=0.02)
        nn.init.normal_(self.W_out.weight, mean=0.0, std=0.02)
    '''

    def forward(self, obs_seq, act_seq, return_hidden=False):
        T, B, _ = obs_seq.size()
        h = torch.zeros(B, self.hidden_dim, device=obs_seq.device)
        y = torch.zeros(B, self.obs_dim, device=obs_seq.device)
        outputs, hiddens = [], []

        # Loop through each timestep of the agent's trajectory
        for t in range(T):
            #################### [ TO DO ] ####################
            # [1 point ] Implement the correct computation of the latent layer h
            # Hint: Look to the paper and see how the authors did this.. you can do it in one line of code
            o_in = self.W_in(obs_seq[t,:,:]) # we learn
            a_in = self.W_act(act_seq[t,:,:])
            h_in = self.W_rec(h)
            bias = self.beta
            g = self.g
            h = g(o_in + a_in + h_in + bias)
            # h =
            ###################################################
            if return_hidden:
                hiddens.append(h.detach().cpu())

            y = torch.sigmoid(self.W_out(h))
            outputs.append(y)

        outputs = torch.stack(outputs)  # (T, B, obs_dim)
        if return_hidden:
            hiddens = torch.stack(hiddens)  # (T, B, hidden_dim)
            return outputs, hiddens
        return outputs

def train_next_step_rnn(model, train_loader, val_loader, num_epochs=20, lr=2e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):
    optimizer = RMSprop(model.parameters(), lr=lr, alpha=0.95, eps=1e-8)
    loss_fn = nn.MSELoss()
    loss_ = [] # to store loss per epoch
    patience = 8
    halfing_counter = 0
    epochs_no_improve = 0
    best_val_loss = float('inf')
    l1_lambda = 1e-3
    l1_lambda = 5e-2
    l1_lambda = 0.05
    l1_lambda = 0.0

    for epoch in range(num_epochs):

        epoch_loss = 0.0
        model.train()
        for obs_seq, act_seq, next_obs_seq in train_loader:
            # obs_seq, act_seq, next_obs_seq shape: (T, B, D)
            obs_seq = obs_seq.permute(1, 0, 2).to(device)  # From (B, T, D) --> (T, B, D)
            act_seq = act_seq.permute(1, 0, 2).to(device)
            next_obs_seq = next_obs_seq.permute(1, 0, 2).to(device)

            optimizer.zero_grad()
            pred = model(obs_seq, act_seq)  # shape: (T, B, obs_dim)
            mse_loss = loss_fn(pred, next_obs_seq)

            loss = mse_loss

            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(train_loader)
        loss_.append(avg_loss)

        # Validation step.
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for obs_seq, act_seq, next_obs_seq in val_loader:
                # obs_seq, act_seq, next_obs_seq shape: (T, B, D)
                obs_seq = obs_seq.permute(1, 0, 2).to(device)  # From (B, T, D) --> (T, B, D)
                act_seq = act_seq.permute(1, 0, 2).to(device)
                next_obs_seq = next_obs_seq.permute(1, 0, 2).to(device)

                pred = model(obs_seq, act_seq)  # shape: (T, B, obs_dim)
                mse_loss = loss_fn(pred, next_obs_seq)

                loss = mse_loss

                val_loss += loss.item()
        val_avg_loss = val_loss / len(val_loader)

        if epoch % 10 == 0:
            print(f"Epoch {epoch+1}: Train Loss = {avg_loss:.6f}, Val Loss = {val_avg_loss:.6f}")

         # Learning rate reduction if no improvement.
        if val_loss < best_val_loss - 5e-5:
            best_val_loss = val_loss
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                for param_group in optimizer.param_groups:
                    param_group['lr'] *= 0.5
                print("Reducing learning rate to", optimizer.param_groups[0]['lr'])
                epochs_no_improve = 0
                halfing_counter+=1
        if halfing_counter >= 5:
            break

    return loss_

# RUN WITH FROZEN PARAMETERS
def extract_hidden_activity_frozen_weights(model, obs_seq, act_seq):
    model.eval()
    # Move input tensors to the same device as the model
    obs_seq = obs_seq.to(device)
    act_seq = act_seq.to(device)
    with torch.no_grad():
        pred, hiddens = model(obs_seq, act_seq, return_hidden=True)
    return pred, hiddens.cpu().numpy()  # shape: (T, hidden_dim)

# Check for NaNs
nan_mask = np.isnan(obs_array)
has_nan = nan_mask.any().item() # Are there any NaNs?
nan_count = nan_mask.sum().item() # Count of NaNs
nan_indices = np.argwhere(nan_mask)
print("Nan sanity check:",has_nan, nan_count, nan_indices)
obs_array= np.nan_to_num(obs_array, nan=0.0)

# Convert to torch tensors
obs_seq = torch.tensor(obs_array[:,:-1], dtype=torch.float32).to(device)      # (num_trials, T-1, obs_dim)
act_seq = torch.tensor(act_array[:,1:], dtype=torch.float32).to(device)          # (num_trials, T-1, act_dim)
next_obs_seq = torch.tensor(obs_array[:,1:], dtype=torch.float32).to(device)  # (num_trials, T-1, act_dim)

# Training dataset for normal predict regime
full_dataset = TensorDataset(obs_seq, act_seq, next_obs_seq)

val_fraction = 0.2  # 20% validation
val_size = int(len(full_dataset) * val_fraction)
train_size = len(full_dataset) - val_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=128, num_workers=0, pin_memory=False)
val_loader = DataLoader(val_dataset, batch_size=128, num_workers=0, pin_memory=False)


# Training dataset for autoencoding regime
full_dataset_autoencoding = TensorDataset(obs_seq, torch.zeros_like(act_seq), obs_seq)

train_dataset_autoencoding, val_dataset_autoencoding = random_split(full_dataset_autoencoding, [train_size, val_size])
train_loader_autoencoding = DataLoader(train_dataset_autoencoding, batch_size=128, num_workers=0, pin_memory=False)
val_loader_autoencoding = DataLoader(val_dataset_autoencoding, batch_size=128, num_workers=0, pin_memory=False)

# check shape of each batch
for batch in train_loader:
    obs, act, next_obs = batch
    print("obs shape:", obs.shape)
    print("act shape:", act.shape)
    print("next_obs shape:", next_obs.shape)
    break  # Just check the first batch

# TRAIN MODEL
obs_dim = obs_seq.shape[-1]
act_dim = act_seq.shape[-1]
hidden_dim = 100
# Initialize model
model = NextStepRNN(obs_dim=obs_dim, act_dim=act_dim, hidden_dim=hidden_dim)

loss_ = train_next_step_rnn(model, train_loader, val_loader, num_epochs=800, lr=0.01)
torch.save(model.state_dict(), "model_final.pth")
print("Saved final predictive-model weights to model_final.pth")

print(VIS_CUTOFF)

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import torch
from matplotlib.colors import LogNorm
from matplotlib.cm import get_cmap


def plot_latent_space_spatial_activity(
        model,
        obs_seq_test,
        act_seq_test,
        x_ratio,
        y_ratio,
        positions_to_use=None,
        nbins=32,
        units_to_plot=100,
        cmap='jet',
        tol=0.125,                 # MAE threshold for “correct”
        acc_batch_size=1000,       # batch size for accuracy batches
        no_visit_eps=1e-5,
        show_dir_counts=False,
        n_angle_bins=36,           # << many angle bins (e.g., 36 = every 10°)
):
    """
    Adds:
      • Occupancy (visits/bin)
      • GLOBAL head-direction compass (polar; n_angle_bins)
      • Step-length histogram + mean/std
      • HD tuning per unit using n_angle_bins
    """

    # --------------------------- helper for tensor → numpy -------------------
    def to_numpy(x):
        return x.detach().cpu().numpy() if isinstance(x, torch.Tensor) else x

    # --------------------------- forward pass & data -------------------------
    if positions_to_use is None:
        positions_to_use = positions  # uses global env variable

    pred, hidden = extract_hidden_activity_frozen_weights(
        model,
        obs_seq_test[0:-2],
        act_seq_test[1:-1],
    )
    pred   = to_numpy(pred).reshape(len(pred), -1)      # (T, 24)
    hidden = to_numpy(hidden).reshape(len(hidden), -1)  # (T, H)

    T, hidden_dim = hidden.shape
    pos_use = np.asarray(positions_to_use[:T])

    # --------------------------- spatial bins --------------------------------
    nbins_x = max(1, int(nbins * x_ratio / max(x_ratio, y_ratio)))
    nbins_y = max(1, int(nbins * y_ratio / max(x_ratio, y_ratio)))

    xs = np.clip((pos_use[:, 0] / x_ratio) * nbins_x, 0, nbins_x-1e-9).astype(int)
    ys = np.clip((pos_use[:, 1] / y_ratio) * nbins_y, 0, nbins_y-1e-9).astype(int)

    occ = np.zeros((nbins_y, nbins_x), np.int32)
    np.add.at(occ, (ys, xs), 1)

    # --------------------------- accuracy counters (batched) -----------------
    obs_np = to_numpy(obs_seq_test)[:, 0]  # (T, 24)

    total_hits   = np.zeros_like(occ)
    correct_hits = np.zeros_like(occ)
    num_correct  = 0

    for i0 in range(0, T-2, acc_batch_size):
        i1 = min(i0 + acc_batch_size, T-2)
        mae = np.abs(pred[i0:i1] - obs_np[i0+2:i1+2]).mean(axis=1)  # (batch,)
        ok  = mae < tol

        num_correct += ok.sum()
        ys_b, xs_b = ys[i0:i1], xs[i0:i1]
        np.add.at(total_hits,   (ys_b, xs_b),         1)
        np.add.at(correct_hits, (ys_b[ok], xs_b[ok]), 1)

    global_acc   = num_correct / (T-2) * 100.0
    accuracy_map = correct_hits / (total_hits + no_visit_eps)
    print(f"Global prediction accuracy: {global_acc:.1f}%  (tol={tol}, MAE)")

    # --------------------------- metrics figure (2x2) ------------------------
    # step lengths from positions (true executed distance per frame)
    step_lengths = np.linalg.norm(np.diff(pos_use, axis=0), axis=1)  # (T-1,)
    step_mean, step_std = step_lengths.mean(), step_lengths.std()

    # head directions (continuous) over time → angle bins
    hd_vecs = np.asarray(Ag.get_history_arrays()['head_direction'][:T], float)
    thetas  = np.arctan2(hd_vecs[:, 1], hd_vecs[:, 0])              # (-pi, pi]
    ang_idx = ((thetas + np.pi) * (n_angle_bins / (2*np.pi))).astype(int) % n_angle_bins
    dir_hist = np.bincount(ang_idx, minlength=n_angle_bins).astype(float)
    if show_dir_counts:
        print(f"Head-dir counts (bins={n_angle_bins}):", dir_hist.astype(int))

    # Occupancy scale choice
    use_log = (occ.max() > 50 * max(1, occ.mean()))
    occ_norm = LogNorm(vmin=max(1, occ[occ>0].min()), vmax=max(1, occ.max())) if use_log else None
    occ_cmap = 'magma'

    fig_acc, axs = plt.subplots(2, 2, figsize=(10, 9))
    (ax_acc, ax_occ), (ax_compass, ax_hist) = axs

    # (1) prediction accuracy
    im0 = ax_acc.imshow(
        accuracy_map, origin='lower', cmap='RdYlGn',
        extent=[0, x_ratio, 0, y_ratio], vmin=0, vmax=1
    )
    ax_acc.set_aspect('equal')
    ax_acc.set_title("Prediction accuracy", fontsize=11)
    ax_acc.set_xlabel("x (m)"); ax_acc.set_ylabel("y (m)")
    fig_acc.colorbar(im0, ax=ax_acc, fraction=0.046, pad=0.04, label="accuracy")

    # (2) occupancy count (visits per bin)
    im1 = ax_occ.imshow(
        occ, origin='lower', cmap=occ_cmap, norm=occ_norm,
        extent=[0, x_ratio, 0, y_ratio]
    )
    ax_occ.set_aspect('equal')
    ax_occ.set_title("Occupancy (visits per bin)", fontsize=11)
    ax_occ.set_xlabel("x (m)"); ax_occ.set_ylabel("y (m)")
    cbar1 = fig_acc.colorbar(im1, ax=ax_occ, fraction=0.046, pad=0.04)
    cbar1.set_label("count" + (" (log)" if use_log else ""))

    # (3) GLOBAL head-direction compass (polar)
    ax_comp = fig_acc.add_subplot(2, 2, 3, projection='polar')
    theta_edges  = np.linspace(0, 2*np.pi, n_angle_bins, endpoint=False)
    theta_centers = theta_edges  # already the bin centers for equal bins
    rr_counts    = dir_hist / (dir_hist.max() + 1e-12)  # normalize for display
    rr_ring      = np.append(rr_counts, rr_counts[0])
    th_ring      = np.append(theta_centers, theta_centers[0])

    ax_comp.plot(th_ring, rr_ring, lw=1.0)
    ax_comp.fill(th_ring, rr_ring, alpha=0.25)
    ax_comp.set_theta_zero_location("E"); ax_comp.set_theta_direction(1)
    # readable 8-way compass ticks
    xticks8 = np.deg2rad([0,45,90,135,180,225,270,315])
    ax_comp.set_xticks(xticks8)
    ax_comp.set_xticklabels(['E','NE','N','NW','W','SW','S','SE'], fontsize=8)
    ax_comp.set_yticks([])
    ax_comp.grid(alpha=0.3, lw=0.3)
    ax_comp.set_title(f"Global head-direction (bins={n_angle_bins})\nN={int(dir_hist.sum())}", fontsize=11)

    # (4) step-length histogram + stats
    ax_hist.hist(step_lengths, bins=40, density=False)
    ax_hist.set_title("Step-length distribution", fontsize=11)
    ax_hist.set_xlabel("distance (m)"); ax_hist.set_ylabel("count")
    ax_hist.axvline(step_mean, ls='--', lw=1)
    ax_hist.text(0.02, 0.95,
                 f"mean = {step_mean:.4f} m\nstd = {step_std:.4f} m\nN = {len(step_lengths)}",
                 transform=ax_hist.transAxes, va='top', fontsize=9,
                 bbox=dict(boxstyle='round,pad=0.4', fc='white', ec='0.3', alpha=0.9))

    fig_acc.suptitle(f"Global prediction accuracy: {global_acc:.1f}% (tol={tol}, MAE)", y=0.995)
    plt.tight_layout()
    plt.show()

    # --------------------------- spatial rate-maps ---------------------------
    act_map = np.zeros((hidden_dim, nbins_y, nbins_x), np.float32)
    for h in range(hidden_dim):
        np.add.at(act_map[h], (ys, xs), hidden[:, h])
    act_map /= (occ + no_visit_eps)

    # --------------------------- HD tuning with n_angle_bins -----------------
    dir_cnt = dir_hist + no_visit_eps  # reuse global counts; for per-unit, we still need ang_idx
    hd_tune = np.zeros((hidden_dim, n_angle_bins), np.float32)
    for h in range(hidden_dim):
        sums = np.bincount(ang_idx, weights=hidden[:, h], minlength=n_angle_bins)
        hd_tune[h] = sums / dir_cnt
    hd_tune /= hd_tune.max(1, keepdims=True) + 1e-12

    theta_edges = np.linspace(0, 2*np.pi, n_angle_bins, endpoint=False)
    theta_ring  = np.append(theta_edges, theta_edges[0])

    # --------------------------- neuron figure -------------------------------
    units_to_plot = min(units_to_plot, hidden_dim)
    cols, rows = 10, int(np.ceil(units_to_plot / 10))
    fig = plt.figure(figsize=(cols * 1.25, rows * 2.2))
    gs  = gridspec.GridSpec(rows*2, cols, hspace=0.65, wspace=0.35)

    for idx in range(units_to_plot):
        r_pair, c = (idx // cols) * 2, idx % cols

        # spatial map
        ax_map = fig.add_subplot(gs[r_pair, c])
        ax_map.imshow(
            act_map[idx], origin='lower', cmap=cmap, interpolation='gaussian',
            extent=[0, x_ratio, 0, y_ratio]
        )
        ax_map.set_aspect('equal'); ax_map.set_xticks([]); ax_map.set_yticks([])
        ax_map.set_title(f"Unit {idx}", fontsize=7, pad=2)

        # HD tuning (polar)
        ax_hd = fig.add_subplot(gs[r_pair + 1, c], projection='polar')
        rr = np.append(hd_tune[idx], hd_tune[idx, 0])
        ax_hd.plot(theta_ring, rr, lw=0.9)
        ax_hd.fill(theta_ring, rr, alpha=0.2)
        ax_hd.set_theta_zero_location("E"); ax_hd.set_theta_direction(1)
        xticks = np.deg2rad([0,45,90,135,180,225,270,315])
        ax_hd.set_xticks(xticks)
        ax_hd.set_xticklabels(['E','NE','N','NW','W','SW','S','SE'], fontsize=6)
        ax_hd.set_yticks([]); ax_hd.grid(alpha=0.3, lw=0.3)

    fig.suptitle(f"Spatial rate-maps + Head-direction tuning (bins={n_angle_bins})", y=0.995)
    plt.tight_layout()
    plt.show()

#################### [ TO DO ] ####################
# Plot the loss across epochs of training
plt.plot(loss_)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()
###################################################

print("VIS_CUTOFF=",VIS_CUTOFF)
############### [ TO DO IN FUNCTION] ##############
# Plot the activity of cells relative toplot_latent_space_spatial_activity
plot_latent_space_spatial_activity(
    model,
    obs_seq_test,
    act_seq_test,
    x_ratio=x_ratio,
    y_ratio=y_ratio,
    positions_to_use=positions
)


###################################################

VIS_CUTOFF= 0.1

"""**trainig new with noise**

"""

rng = np.random.default_rng(seed=42)

obs_array=masked_copy_noisy(obs_array_orig, 1,noise_std=0, rng=rng,zero_colours=[],noise_phase="pre",dist_power=4)
obs_array_test= masked_copy_noisy(obs_array_test_orig,1,noise_std=0.0, rng=rng,zero_colours=[],noise_phase="pre",dist_power=4)
# Check for NaNs
nan_mask = np.isnan(obs_array_test)
has_nan = nan_mask.any().item() # Are there any NaNs?
nan_count = nan_mask.sum().item() # Count of NaNs
nan_indices = np.argwhere(nan_mask)
print("Nan sanity check:",has_nan, nan_count, nan_indices)
obs_array_test= np.nan_to_num(obs_array_test, nan=0.0)

# Convert to torch tensors
obs_seq_test = torch.tensor(obs_array_test, dtype=torch.float32).unsqueeze(1)
# Check for NaNs
nan_mask = np.isnan(obs_array)
has_nan = nan_mask.any().item() # Are there any NaNs?
nan_count = nan_mask.sum().item() # Count of NaNs
nan_indices = np.argwhere(nan_mask)
print("Nan sanity check:",has_nan, nan_count, nan_indices)
obs_array= np.nan_to_num(obs_array, nan=0.0)

# Convert to torch tensors - now handling 2D actions
obs_seq = torch.tensor(obs_array[:,:-1], dtype=torch.float32).to(device)      # (num_trials, T-1, obs_dim)
act_seq = torch.tensor(act_array[:,1:], dtype=torch.float32).to(device)          # (num_trials, T-1, 2) - 2D actions
next_obs_seq = torch.tensor(obs_array[:,1:], dtype=torch.float32).to(device)  # (num_trials, T-1, obs_dim)

# Training dataset for normal predict regime
full_dataset = TensorDataset(obs_seq, act_seq, next_obs_seq)

val_fraction = 0.2  # 20% validation
val_size = int(len(full_dataset) * val_fraction)
train_size = len(full_dataset) - val_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=128, num_workers=0, pin_memory=False)
val_loader = DataLoader(val_dataset, batch_size=128, num_workers=0, pin_memory=False)


# Training dataset for autoencoding regime
full_dataset_autoencoding = TensorDataset(obs_seq, torch.zeros_like(act_seq), obs_seq)

train_dataset_autoencoding, val_dataset_autoencoding = random_split(full_dataset_autoencoding, [train_size, val_size])
train_loader_autoencoding = DataLoader(train_dataset_autoencoding, batch_size=128, num_workers=0, pin_memory=False)
val_loader_autoencoding = DataLoader(val_dataset_autoencoding, batch_size=128, num_workers=0, pin_memory=False)

# check shape of each batch
for batch in train_loader:
    obs, act, next_obs = batch
    print("obs shape:", obs.shape)
    print("act shape:", act.shape)  # Should now be (..., 2) instead of (..., 9)
    print("next_obs shape:", next_obs.shape)
    break  # Just check the first batch
import torch

# TRAIN MODEL - dimensions will be automatically inferred
obs_dim = obs_seq.shape[-1]
act_dim = act_seq.shape[-1]  # Will now be 2 instead of 9
hidden_dim = 100
# Initialize model
model = NextStepRNN(obs_dim=obs_dim, act_dim=act_dim, hidden_dim=hidden_dim)

loss_ = train_next_step_rnn(model, train_loader, val_loader, num_epochs=800, lr=0.01)
torch.save(model.state_dict(), "model_final.pth")
print("Saved final predictive-model weights to model_final.pth")


#################### [ TO DO ] ####################
# Plot the loss across epochs of training
plt.plot(loss_)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()
###################################################

print("VIS_CUTOFF=",VIS_CUTOFF)
############### [ TO DO IN FUNCTION] ##############
# Plot the activity of cells relative toplot_latent_space_spatial_activity
plot_latent_space_spatial_activity(
    model,
    obs_seq_test,
    act_seq_test,
    x_ratio=x_ratio,
    y_ratio=y_ratio,
    positions_to_use=positions
)

"""**training new no noise**

**New ENV**

**Retrainig**
"""



